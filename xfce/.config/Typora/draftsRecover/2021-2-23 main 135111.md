---
author:
- "-----"
date: November 2020
title: Programming Fundamentals
---

# Time Complexity

Running of an algorithm depends on various factors:

-   Single vs Multiple Processor

-   Read & Write Speed

-   32 bit vs 64 bit architecture

-   Configuration of the Machine

-   Input $\rightarrow$ Evaluation of Time as a Function of Input

#### Model Machine:

-   Single Processor

-   32 bit

-   Sequential Execution

-   Takes 1-unit of time for any arithmetic & logical operations

-   Takes 1-unit of time for any assignment & return statements

## Time Complexity Calculation

[**Evaluation:1**]{.ul}

``` {.c++ language="C++"}
// sum of two numbers
    
    sum(num_1, num_2):
        return (num_1 + num_2)
```

*Time Taken:* One arithmetic instruction & one return instruction Hence,
irrespective of inputs the function will always take 2-units of time
$T(n) = 2$, **Constant Function**.

#### 

[**Evaluation:2**]{.ul}

``` {.c++ language="C++"}
// sum of a list
    
    sum(list, size_of_list):
        total = 0                     // 1 unit cost operation, 1 time
        for i -> 0 to size_of_list:   // 2 unit cost operation, (n+1) times
            total = total + list[i]   // 2 unit cost operation, n times
        return total                  // 1 unit cost operation, 1 time
```

$T(n) = 4(n + 1)$, **Linear Function**.

#### 

[**Evaluation:2**]{.ul} Time complexity for the summation of a matrix
$\rightarrow$ A form of quadratic equation.

$T(n) = an^2 + bn + c$, **Quadratic Function**.

## Asymptomatic Notations

Mapping of running time into some sets. For example, the below two
functions can be mapped into a single set, as $n \rightarrow \infty$,
the lower-order terms can be ignored.

$$\begin{aligned}
    T_1(n) & = 6n^2 + 8n + 4 \nonumber \\
    T_2(n) & = 6n^2 + 8 \nonumber\end{aligned}$$

Hence, when $n \rightarrow \infty$, we can say that
$T_1(n) \simeq T_2(n)$. The grouping of similar functions when
$n \rightarrow \infty$ is the intuition behind sets.

#### 

[**Big-$\mathcal{O}$ Notation:**]{.ul} There exists a function $f(n)$,
and two constants, $c$ & $n_0$, such that,
$$f(n) \leq c g(n), \text{  for all, } n \geq n_0$$

Example: $$\begin{aligned}
    f(n) & = 8n^2 + 2n + 1 \nonumber \\
    g(n) & = n^2 \nonumber\end{aligned}$$

![Big-$\mathcal{O}$ Property. This picture shows that the function
$f(n)$ is always $\leq 8n^22$, when $n \geq n_0$, given
$[n_0 = 1, c = 8]$. After, $n_0, cg(n)$, is always greater than $f(n)$.
From here we can say that Big-$\mathcal{O}$ is basically upper bound of
rate of growth of time.](Big_O.png){#fig:big_o width="50%"}

#### 

[**$\Omega$ Notation**]{.ul}: There exists a function $f(n)$, and two
constants, $c$ & $n_0$, such that,
$$c g(n) \leq f(n), \text{  for all, } n \geq n_0$$

Example: $$\begin{aligned}
    f(n) & = 5n^2 + 2n + 1 \nonumber \\
    g(n) & = n^2 \nonumber\end{aligned}$$

![$\Omega$ Property. This picture shows that the function $f(n)$ is
always $\geq 5n^2$, when $n \geq 0$, given $[n_0 = 0, c = 5]$. After,
$n_0, cg(n)$, is always less than $f(n)$. From here we can say that
$\Omega$ is basically lower bound of rate of growth of
time.](Omega_notation.png){#fig:omega_n width="50%"}

#### 

$\Theta$ Notation There exists a function $f(n)$, and
constants, $c_1, c_2$ & $n_0$, such that,
$$c_1 g(n) \leq f(n) \leq c_2 g(n), \text{  for all, } n \geq n_0$$

Example: $$\begin{aligned}
    f(n) & = 5n^2 + 2n + 1 \nonumber \\
    g(n) & = n^2 \nonumber\end{aligned}$$

![$\Theta$ Property. This picture shows that the function $f(n)$ always
satisfies $5n^2 \leq f(n) \leq 8n^2$ when $n \geq n_0$, given
$[n_0 = 1, c_1 = 5, c_2 = 8]$. After, $n_0, c_1g(n)$, is always less
than $f(n)$ and $c_1g(n)$ is always greater. From here we can say that
$\Theta(n^2)$ is basically the tight bound & it's always recommended to
evaluate the run-time of an algorithm using the
tight-bound](Theta_notation.png){#fig:theta_n width="50%"}